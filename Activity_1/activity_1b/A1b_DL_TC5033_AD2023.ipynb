{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "DATA_PATH = './asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))   # train data\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))   # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the first 5 rows of the train data \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code removes the label column from the data and converts the data into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])   # training data labels\n",
    "y_val = np.array(valid_df['label'])     # validation data labels\n",
    "\n",
    "del train_df['label']                   # remove the label column from the train data\n",
    "del valid_df['label']                   # remove the label column from the validation data\n",
    "\n",
    "x_train = train_df.values.astype(np.float32)    # convert the train data to a numpy array\n",
    "x_val = valid_df.values.astype(np.float32)      # convert the validation data to a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function that will split the validation test into a validation set and a test set. We will use the validation set to tune the hyperparameters and the test set to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    This function splits the previously loaded validation set into valition and test sets.\n",
    "    args: x: numpy array of images\n",
    "          y: numpy array of labels\n",
    "          pct: percentage of data to be used for validation\n",
    "          shuffle: whether to shuffle the data before splitting\n",
    "    '''\n",
    "    if shuffle:\n",
    "        idx = np.arange(len(x))     # create an array of indices\n",
    "        np.random.shuffle(idx)      # shuffle the indices\n",
    "        x = x[idx]                  # shuffle the array of images\n",
    "        y = y[idx]                  # shuffle the array of labels\n",
    "    # split the images and labels into training and validation sets\n",
    "    return x[:int(len(x)*pct)], y[:int(len(y)*pct)], x[int(len(x)*pct):], y[int(len(y)*pct):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation set into validation and test\n",
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data, we now verify that the shape of the training, validation, and test sets are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: ((27455, 784), (27455,))\n",
      "Validation set: ((3586, 784), (3586,))\n",
      "Test set: ((3586, 784), (3586,))\n"
     ]
    }
   ],
   "source": [
    "print(f'Training set: {x_train.shape, y_train.shape}')\n",
    "print(f'Validation set: {x_val.shape, y_val.shape}')\n",
    "print(f'Test set: {x_test.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is checks the number of unique labels in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "alphabet=list(string.ascii_lowercase)  # create a list of all the alphabets\n",
    "alphabet.remove('j')                   # remove the letter j from the list\n",
    "alphabet.remove('z')                   # remove the letter z from the list\n",
    "\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function normalizes the data by calculating the **Z-score:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization Z-score\n",
    "def normalise(x_mean, x_std, x_data):\n",
    "    '''\n",
    "    This function returns the Z-score of the input data.\n",
    "    args: x_mean: mean of the input data\n",
    "          x_std: standard deviation of the input data\n",
    "          x_data: input data\n",
    "    '''\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(x_train)   # calculate the mean of the training set\n",
    "x_std = np.std(x_train)     # calculate the standard deviation of the training set\n",
    "\n",
    "x_train = normalise(x_mean, x_std, x_train)     # normalize the training set\n",
    "x_val = normalise(x_mean, x_std, x_val)         # normalize the validation set\n",
    "x_test = normalise(x_mean, x_std, x_test)       # normalize the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing the data, we should have a mean of 0 and a standard deviation of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.6268384e-06, 0.99999946)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the mean and standard deviation of the normalized training set\n",
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots sample images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alphabet(image):\n",
    "    '''\n",
    "    This function plots the image of the alphabet list\n",
    "    args: image: image to be plotted\n",
    "    '''\n",
    "    plt.imshow(image.reshape(28,28), cmap='gray')   # reshape the image to 28x28 and plot it\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot random images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image belongs to the letter n\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnd_idx = np.random.randint(len(y_test))   # create a random index\n",
    "\n",
    "print(f'This image belongs to the letter {alphabet[y_test[rnd_idx]]}')\n",
    "plot_alphabet(x_test[rnd_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations for our model:\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid memory issues during training, we will split the training data into mini-batches. The following function creates mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch generator\n",
    "def minibatch(mb_size, x, y, shuffle=True):\n",
    "    '''\n",
    "    This function creates the mini-batches.\n",
    "    args: mb_size: mini-batch size\n",
    "          x: numpy array of images\n",
    "          y: numpy array of labels\n",
    "          shuffle: whether to shuffle the data before splitting\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Sample Qty. Error'    # check if the number of images and labels are equal\n",
    "    total_data = x.shape[0]                                 # get the total number of data\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.arange(total_data)        # create an array of indices\n",
    "        np.random.shuffle(idxs)             # shuffle the indices\n",
    "        x = x[idxs]                         # shuffle the array of images\n",
    "        y = y[idxs]                         # shuffle the array of labels\n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))   # return the mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear, ReLU and Sequential Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a tensor from a numpy array\n",
    "class np_tensor(np.ndarray): pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Linear()` class initializes the weights and biases for each layer. The forward pass is implemented using the `__call__` function. The `backward` function performs the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        This function initializes the parameters using Kaiming He initialization\n",
    "        args: input_size: number of input features\n",
    "              output_size: number of output features    \n",
    "        '''\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)  # initialize the weights using Kaiming He\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)                                        # initialize the biases as zeros\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        This function performs the forward pass of the linear layer\n",
    "        args: X: input data\n",
    "        '''\n",
    "        Z = self.W @ X + self.b         # calculate the linear transformation\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, Z):\n",
    "        '''\n",
    "        This function performs the backward pass of the linear layer\n",
    "        args: X: input data\n",
    "              Z: output data of the forward pass\n",
    "        '''\n",
    "        X.grad = self.W.T @ Z.grad                              # calculate the gradients of the inputs    \n",
    "        self.W.grad = Z.grad @ X.T                              # calculate the gradients of the weights\n",
    "        self.b.grad = np.sum(Z.grad, axis=1, keepdims=True)     # sum the gradients of the biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `ReLU()` class we also implement the forward and backward passes, using the `__call__` and `backward` functions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, Z):\n",
    "        '''\n",
    "        This function performs the ReLU activation\n",
    "        args: Z: input data from the previous linear layer\n",
    "        '''\n",
    "        return np.maximum(0, Z)     # Returns zero if the input is negative or the input itself if it is positive\n",
    "    \n",
    "    def backward(self, Z, A):\n",
    "        '''\n",
    "        This function performs the backward pass of the ReLU activation function\n",
    "        args: Z: input data from the previous linear layer\n",
    "              A: output data of the ReLU activation function\n",
    "        ''' \n",
    "        Z.grad = A.grad.copy()      # Backward pass of the ReLU activation function\n",
    "        Z.grad[Z <= 0] = 0          # Set the gradients to zero where the inputs are negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sequential_layers()` class is a container for the neural network layers. It is used to stack the layers in the order defined by the user. The `__call__` function is used to perform the forward pass. The `backward` function is used to perform the backward pass. Weights and biases are updated using the `update` function, and the `predict` function returns the predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        This function initializes the layers\n",
    "        args: layers: list of layers\n",
    "        '''\n",
    "        self.layers = layers    # initialize the layers\n",
    "        self.x = None           # initialize the input\n",
    "        self.outputs = {}       # initialize the outputs\n",
    "   \n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        This function performs the forward pass of the network\n",
    "        args: X: input data\n",
    "        '''\n",
    "        self.x = X                                      # set the input of the first layer\n",
    "        self.outputs['l0'] = self.x                     # set the output of the first layer\n",
    "        for i, layer in enumerate(self.layers, 1):      # loop through the layers\n",
    "            self.x = layer(self.x)                      # forward pass for each layer\n",
    "            self.outputs['l'+str(i)] = self.x           # set the output of each layer\n",
    "        return self.x\n",
    "   \n",
    "    def backward(self):\n",
    "        '''\n",
    "        This function performs the backward pass of the network\n",
    "        '''\n",
    "        for i in reversed(range(len(self.layers))):                                         # loop through the layers in reverse order\n",
    "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])   # backward pass for each layer\n",
    "    \n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        '''\n",
    "        This function updates the weights and biases of the network\n",
    "        args: learning_rate: learning rate or step size\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): continue                # skip the ReLU layer\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad    # update the weights\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad    # update the biases\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function predicts the class of the input\n",
    "        args: X: input data\n",
    "        '''\n",
    "        return np.argmax(self.__call__(X))      # return the predicted class   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **Softmax** function to calculate the probabilities of each class. The **cross-entropy** loss is then computed using the predicted probabilities and the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    '''\n",
    "    This function computes the softmax and cross-entropy loss\n",
    "    args: x: numpy array of scores\n",
    "          y: numpy array of labels\n",
    "    '''\n",
    "    batch_size = x.shape[1]         # get the batch size\n",
    "    exp_scores = np.exp(x)          # calculate the exponential of the scores\n",
    "    probs = exp_scores / exp_scores.sum(axis=0)   # calculate the softmax probabilities\n",
    "    preds = probs.copy()                          # make a copy of the probabilities\n",
    "\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]  # get the probabilities of the correct class\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size         # calculate the cross-entropy loss for the batch\n",
    "\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1     # subtract 1 from the probabilities of the correct class\n",
    "    x.grad = probs.copy()                              # define the gradients of the inputs\n",
    "\n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we first perform the forward pass, then calculate the loss, and finally update weights and biases in the backward pass. We also calculate the accuracy of the model on the training set and the validation set. The training loop is run for the number of epochs specified by the user.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size = 128, learning_rate= 1e-3):\n",
    "    '''\n",
    "    This function trains the model\n",
    "    args: model: neural network model to be trained\n",
    "          epochs: number of epochs or iterations\n",
    "          mb_size: mini-batch size\n",
    "          learning_rate: learning rate or step size\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(minibatch(mb_size, x_train, y_train)):\n",
    "            scores = model(x.T.view(np_tensor))                 # calculate the scores\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "            model.backward()                                     # calculate the gradients\n",
    "            model.update(learning_rate)                          # update the parameters\n",
    "        print(f'Cost: {cost}, Accuracy: {accuracy(x_val, y_val, mb_size)}')   # print the cost and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the accuracy of the model by comparing the predicted labels with the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    '''\n",
    "    This function calculates the accuracy of the model\n",
    "    args: x: numpy array of images\n",
    "          y: numpy array of labels\n",
    "          mb_size: mini-batch size\n",
    "    '''\n",
    "    correct = 0           \n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(minibatch(mb_size, x, y)):\n",
    "        pred = model(x.T.view(np_tensor))                           # predict the class of the input\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())   # calculate the number of correct predictions\n",
    "        total += pred.shape[1]                                      # calculate the total number of predictions\n",
    "        return correct / total                                      # return the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture:\n",
    "\n",
    "- Input layer: 784 neurons (28 x 28 images)\n",
    "- Hidden layer 1: 200 neurons, ReLU activation function\n",
    "- Hidden layer 2: 200 neurons, ReLU activation function\n",
    "- Hidden layer 3: 200 neurons, ReLU activation function\n",
    "- Output layer: 24 neurons (24 classes), Softmax activation function\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- batch size = 64\n",
    "- learning rate = 1 e-4\n",
    "- number of epochs = 20\n",
    "\n",
    "Justification:\n",
    "\n",
    "Since we have a relatively small training set (27,455 samples), we will use a small batch size of 64. We will experiment with a learning rate of 1 e-4, which is a relatively small learning rate. Our model will have 3 hidden layers with 200 neurons each, we want to avoid overfitting by having a very complex model.  Fianlly, we will train the model for 20 epochs, and our aim is to achieve an accuracy greater than 70% on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential_layers([Linear(784, 200), ReLU(), \n",
    "                           Linear(200, 200), ReLU(),\n",
    "                           Linear(200, 200), ReLU(), \n",
    "                           Linear(200, 200), ReLU(), \n",
    "                           Linear(200, 24)\n",
    "                          ])    \n",
    "mb_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model using our `train` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.8683193098926842, Accuracy: 0.59375\n",
      "Cost: 0.1961563103766008, Accuracy: 0.703125\n",
      "Cost: 0.0944811528936461, Accuracy: 0.65625\n",
      "Cost: 0.06082201521217852, Accuracy: 0.703125\n",
      "Cost: 0.038926449323169805, Accuracy: 0.8125\n",
      "Cost: 0.025638386568766155, Accuracy: 0.71875\n",
      "Cost: 0.016340263900647932, Accuracy: 0.75\n",
      "Cost: 0.01623095192772547, Accuracy: 0.765625\n",
      "Cost: 0.01981931465695581, Accuracy: 0.828125\n",
      "Cost: 0.01458815289774348, Accuracy: 0.703125\n",
      "Cost: 0.010911535373564402, Accuracy: 0.75\n",
      "Cost: 0.011421879570158918, Accuracy: 0.75\n",
      "Cost: 0.012802263575828083, Accuracy: 0.703125\n",
      "Cost: 0.005164264890476534, Accuracy: 0.828125\n",
      "Cost: 0.0056787671644060265, Accuracy: 0.859375\n",
      "Cost: 0.005414499042960726, Accuracy: 0.8125\n",
      "Cost: 0.005353417783073594, Accuracy: 0.703125\n",
      "Cost: 0.005586048398644634, Accuracy: 0.75\n",
      "Cost: 0.002852139498294403, Accuracy: 0.75\n",
      "Cost: 0.003246812666432363, Accuracy: 0.828125\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.796875\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy(x_test, y_test, mb_size)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value is: h \n",
      "The actual value is: h\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))                # create a random index\n",
    "plot_alphabet(x_test[idx].reshape(28,28))           # plot the image\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))    # predict the class of the image\n",
    "\n",
    "# Print the test image, predicted value and actual value\n",
    "print(f'The predicted value is: {alphabet[pred]} \\nThe actual value is: {alphabet[y_test[idx]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
