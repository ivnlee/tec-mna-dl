{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Text Generation\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8ff971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'     # for mac with apple silicon\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3288ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer function will be used to convert the text into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4c7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code will create a vocabulary of tokens from the train dataset.\n",
    "\n",
    "The parameter `specials` is used to specify special tokens that will be added to the vocabulary. In this case, we are adding four special tokens:\n",
    "\n",
    " * `<unk>`: for unknown words, not found in the training dataset.\n",
    " * `<pad>`: for padding sequences to the same length.\n",
    " * `<bos>`: for marking the begining of a sequence.\n",
    " * `<eos>`: for marking the end of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2cb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `data_process` returns the input tensor and the target tensor that will be used to train the LSTM model.\n",
    "\n",
    "Arguments: \n",
    "* `raw_text_iter` iterator over the text dataset.\n",
    "* `seq_length` length of the sequence for the LSTM model, default = 50.\n",
    "\n",
    "First, the function will tokenize each item using `tokenizer` then convert the tokens to their corresponding indices in the vocabulary using `vocab`, and then convert the resulting list of indices to a tensor.\n",
    "\n",
    "Then, the function will remove empty tensors and concatenate the remaining tensors into a single tensor using `torch.cat`.\n",
    "\n",
    "Finally, the function will return two tensors: the first tensor is created by trimming the `data` tensor to a legth that is a multiple of the sequence length and reshaping it into a 2D tensor. The second tensor is created by shifting the `data` tensor by one position to the right, trimming its length to the same length as the first tensor and reshaping into a 2D tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134b832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]  # tokenize the text and convert to tensor\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))                             # remove empty tensors\n",
    "\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),      # trim the data to have a length that's multiple of the sequence length            \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))   # shift the data by one to the right to create the target    \n",
    "\n",
    "# # Create tensors for the training set\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TensorDataset` class is used to create a dataset of tensors. The train dataset will be used to create a data loader that will feed the data to the LSTM model during training.\n",
    "\n",
    "*Note: For this particular application of text generation, we will not use the validation or test datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b54c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)      \n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40999 4288 4837\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # choose a batch size that fits your computation resources\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c63b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel(nn.Module):                                                        # inherit from PyTorch's nn.Module\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):             # initialize the class\n",
    "        super(LSTMModel, self).__init__()                                            # call the constructor of the parent class\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)                       # embedding layer\n",
    "        self.hidden_size = hidden_size                                               # hidden size parameter\n",
    "        self.num_layers = num_layers                                                 # number of layers parameter\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)   # LSTM layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)                                 # fully connected layer maps output to vocabulary size\n",
    "\n",
    "    def forward(self, text, hidden):                      # forward pass method  \n",
    "        embeddings = self.embeddings(text)                  # convert input text to embeddings\n",
    "        output, hidden = self.lstm(embeddings, hidden)      # pass embeddings and hidden state to LSTM layer\n",
    "        decoded = self.fc(output)                           # pass output of LSTM layer to fully connected layer\n",
    "        return decoded, hidden                              # return output and hidden state\n",
    "\n",
    "    def init_hidden(self, batch_size):                                                 # hidden state initialization method\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),   # initialize hidden state to zeros\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))   # initialize cell state to zeros\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)  # vocabulary size\n",
    "emb_size = 50            # embedding size\n",
    "neurons = 300            # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 2           # the number of nn.LSTM layers\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)  # instantiate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embeddings): Embedding(28785, 50)\n",
       "  (lstm): LSTM(50, 300, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=300, out_features=28785, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LSTM model will consist of the following components:\n",
    "\n",
    "1. An embedding layer that will convert the input tokens into dense vectors of size 50.\n",
    "2. Two LSTM layers with 300 hidden units each.\n",
    "3. A fully connected layer that will map the output of the LSTM layer to the vocabulary size, which is 28785 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215eabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser):\n",
    "    '''\n",
    "    The following are possible instructions you may want to conside for this function.\n",
    "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
    "    as long as you train your model correctly.\n",
    "        - loop through specified epochs\n",
    "        - loop through dataloader\n",
    "        - don't forget to zero grad!\n",
    "        - place data (both input and target) in device\n",
    "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
    "        - run the model\n",
    "        - compute the cost or loss\n",
    "        - backpropagation\n",
    "        - Update paratemers\n",
    "        - Include print all the information you consider helpful\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    model = model.to(device=device)                         # move the model parameters to GPU\n",
    "    model.train()                                           # put the model in training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (data, targets) in enumerate((train_loader)):            # loop through the data loader\n",
    "            data = data.to(device=device)                               # place input data in device\n",
    "            targets = targets.to(device=device)                         # place targets in device\n",
    "            \n",
    "            hidden = model.init_hidden(batch_size)                      # initialize hidden state vectors\n",
    "            outputs, hidden = model(data, hidden)                       # perform the forward pass\n",
    "            loss = F.cross_entropy(outputs.view(-1, vocab_size), targets.view(-1))  # compute the loss  \n",
    "\n",
    "            optimiser.zero_grad()                  # reset gradients to zero\n",
    "            loss.backward()                        # perform the backward pass\n",
    "            optimiser.step()                       # update the parameters\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9c84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 6.827858924865723\n",
      "Epoch: 2, Loss: 6.5476813316345215\n",
      "Epoch: 3, Loss: 6.337130069732666\n",
      "Epoch: 4, Loss: 6.299001693725586\n",
      "Epoch: 5, Loss: 6.213036060333252\n",
      "Epoch: 6, Loss: 6.04749870300293\n",
      "Epoch: 7, Loss: 5.978874683380127\n",
      "Epoch: 8, Loss: 5.8547139167785645\n",
      "Epoch: 9, Loss: 5.894222259521484\n",
      "Epoch: 10, Loss: 5.832911491394043\n",
      "Epoch: 11, Loss: 5.6278886795043945\n",
      "Epoch: 12, Loss: 5.5529561042785645\n",
      "Epoch: 13, Loss: 5.569584846496582\n",
      "Epoch: 14, Loss: 5.3070068359375\n",
      "Epoch: 15, Loss: 5.308558464050293\n"
     ]
    }
   ],
   "source": [
    "# Call the train function\n",
    "# Hyperparameters\n",
    "lr = 0.0005             # learning rate\n",
    "epochs = 15             # number of epochs  \n",
    "\n",
    "#Adam optimizer\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "# Train the model\n",
    "train(model, epochs, optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be used to generate text using the trained LSTM model, it takes four arguments:\n",
    "\n",
    "* `model`: the trained LSTM model.\n",
    "* `start_text`: the initial text to start the generation.\n",
    "* `num_words`: the number of words to generate.\n",
    "* `temperature`: a parameter used to control the randomness of the generated text.\n",
    "\n",
    "The function first sets the model to evaluation mode, then it tokenizes the `start_text` and assigns it to the variable `words`, the hidden state of the LSTM is initialized with a batch size of 1.\n",
    "\n",
    "The `for` loop will iterate over the number of words to generate, in each iteration, the following steps will be performed:\n",
    "\n",
    "1. The `words` sequence is converted to a tensor of their corresponding indices in the vocabulary.\n",
    "2. The tensor is fed into the model to get the output and the hidden state.\n",
    "3. Get the logits of the last predicted word.\n",
    "4. Apply softmax to the logits to get the probabilities (the logits are divided by the `temperature` parameter), then detach the resulting tensor, move it to the CPU and convert to a NumPy array.\n",
    "5. Randomly select a word index from the array according to the probabilities.\n",
    "6. Append the selected word to the `words` sequence.\n",
    "7. Finally, join the words in the `words` sequence to get the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8667411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like pneumonia and a phrase on all cosmetics , adrien and folk similar , killing in 1895 . by the blue dance system , trinsey , 2 @ . @ 3 , buildup up the 2004 aviation of the anime raid . for their pitt , a 2010 team suggests that several cemetery ' s occasions for her call emerging . they showed a talking by eight , veronica and crickets of ireland . his recommendation residues , using organ his delay [ tom ] ) . a collection , stating that he feared the role in this flyby together in\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):  \n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    for i in range(0, num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)\n",
    "    \n",
    "    \n",
    "\n",
    "# Generate some text\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2884543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name is the scientific rock . = = background = = on his week after its album began twice from the <unk> . additionally , the tropical final length of u , <unk> <unk> , crew <unk> áƒ» and additional features <unk> <unk> . = = thompson and jews of 160 ,\n"
     ]
    }
   ],
   "source": [
    "start_text = \"Hello my name is\"\n",
    "\n",
    "print(generate_text(model, start_text=start_text, num_words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78eabe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the city of us days during other german level , and south centuries until 754 the space was voted the cathedral compared in late 1975 . the hurricane are a special determined using\n"
     ]
    }
   ],
   "source": [
    "start_text = \"In the city of\"\n",
    "\n",
    "print(generate_text(model, start_text=start_text, num_words=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb126a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the president said their calls dropped about pursuit ( 48 ) ' s twenty @-@ century community . the main beliefs were improved\n"
     ]
    }
   ],
   "source": [
    "start_text = \"The president said\"\n",
    "\n",
    "print(generate_text(model, start_text=start_text, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
